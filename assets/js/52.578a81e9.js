(window.webpackJsonp=window.webpackJsonp||[]).push([[52],{476:function(e,r,_){"use strict";_.r(r);var t=_(15),v=Object(t.a)({},(function(){var e=this,r=e._self._c;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("p",[e._v("本文是一篇轻分享")]),e._v(" "),r("h2",{attrs:{id:"大模型并不神奇"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#大模型并不神奇"}},[e._v("#")]),e._v(" 大模型并不神奇")]),e._v(" "),r("p",[e._v('很多人听到"大模型"这个词可能会觉得很神秘，其实，LLM 就是'),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("神经网络"),r("OutboundLink")],1),e._v("，只是很大的神经网络，相对传统神经网络，大就是它的特点。我们用一个"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("压缩算法"),r("OutboundLink")],1),e._v("的简单例子来帮助理解这个巨大的神经网络。")]),e._v(" "),r("h3",{attrs:{id:"原始的压缩算法"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#原始的压缩算法"}},[e._v("#")]),e._v(" 原始的压缩算法")]),e._v(" "),r("ul",[r("li",[e._v("原始数据")])]),e._v(" "),r("p",[e._v("首先，我们来看一下传统的压缩算法。假设我们手头有一段原始数据：AAABBBB。这些数据在未压缩的情况下占用1GB的存储空间。")]),e._v(" "),r("ul",[r("li",[e._v("压缩过程")])]),e._v(" "),r("p",[e._v("接下来，压缩算法会使用固定的规则，寻找数据中的重复模式，比如这里的连续'A'和'B'。通过这种方式，压缩算法可以将AAABBBB压缩成一个更短的形式：3A4B。")]),e._v(" "),r("ul",[r("li",[e._v("压缩后")])]),e._v(" "),r("p",[e._v("这样一来，压缩后的数据仅占用500MB的存储空间。通过更短的字符表示原始的数据，我们大大减少了存储需求。")]),e._v(" "),r("p",[e._v("需要用这个数据的时候，也是固定的规则，把数据解压还原成 AAA BBBB")]),e._v(" "),r("ul",[r("li",[e._v("压缩率")])]),e._v(" "),r("p",[e._v("压缩算法的一个重要特点是速度快和但压缩率低。这个例子中，我们通过寻找和表示数据中的重复模式，大幅减少了数据的存储需求。这个比例通常不会很高，但却是无损的。")]),e._v(" "),r("p",[r("img",{attrs:{src:"https://pic2.zhimg.com/80/v2-142685b0b5fc88ff80421804a4bfc9bb_720w.webp",alt:""}})]),e._v(" "),r("h3",{attrs:{id:"大模型的压缩"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#大模型的压缩"}},[e._v("#")]),e._v(" 大模型的压缩")]),e._v(" "),r("p",[e._v("我们来看一下大模型的也是类似压缩过程。")]),e._v(" "),r("ul",[r("li",[e._v("原始数据")])]),e._v(" "),r("p",[e._v("首先，我们有大量需要学习的数据。就像我们刚才提到的传统压缩算法，大模型在训练之前也需要处理大量的原始数据，它可能包含世界上大部分书籍，网络中数亿语料。假设在海量数据中，我们有一首诗“对酒当歌，"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E4%BA%BA%E7%94%9F%E5%87%A0%E4%BD%95&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("人生几何"),r("OutboundLink")],1),e._v("”，这些全世界搜罗的各类文本总的一起可能占用高达10T的存储空间，比如 llama3 使用的预训练数据高达15T。")]),e._v(" "),r("ul",[r("li",[e._v("压缩过程")])]),e._v(" "),r("p",[e._v("然后，大模型通过训练，不断迭代优化、调整参数，从数据中“学习”各种复杂的模式。这个过程有点像我们前面提到的压缩算法，只不过大模型的压缩更加复杂和智能。它通过调整模型的参数——包括权重和偏置——来捕捉数据中的模式和规律。也就是上一页 ppt 提到的重复部分。")]),e._v(" "),r("ul",[r("li",[e._v("压缩后")])]),e._v(" "),r("p",[e._v("最终，我们得到了一个经过训练的模型，它只需要10GB的存储空间。这就像是我们从原始的10TB数据中提取出的“压缩”版本。这个压缩版本包含了所有重要的信息，以便模型能够高效地进行推理和预测。")]),e._v(" "),r("ul",[r("li",[e._v("压缩率")])]),e._v(" "),r("p",[e._v("值得注意的是，这种过程相对较慢，需要耗费大量的时间和计算资源，但压缩率非常高。")]),e._v(" "),r("p",[e._v("我们可以看到，一个 10T的预料，可能被压缩到一个 10G 大小的模型中。llama3 8b 就是一个非常稠密的模型。")]),e._v(" "),r("p",[r("img",{attrs:{src:"https://pic2.zhimg.com/80/v2-45f457743d751010e10882b6f9b7e061_720w.webp",alt:""}})]),e._v(" "),r("h3",{attrs:{id:"llm-不仅仅压缩了字符串"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#llm-不仅仅压缩了字符串"}},[e._v("#")]),e._v(" LLM 不仅仅压缩了字符串")]),e._v(" "),r("p",[e._v("实际上，更重要的是它能够学习字符串之间的相关性，也就是抽象规律，甚至能够推理出原始数据中不存在的事物。")]),e._v(" "),r("ul",[r("li",[e._v("抽象模式学习")])]),e._v(" "),r("p",[e._v("我们来看一个例子。在训练过程中，大模型会学习大量类似的数据。例如，输入的数据可能包含“苏格拉底是人，人会死，苏格拉底会死”这样的句子。通过不断迭代优化，大模型能够学习到这些数据中的抽象模式，比如“"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E4%B8%89%E6%AE%B5%E8%AE%BA&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("三段论"),r("OutboundLink")],1),e._v("”这样的逻辑关系。")]),e._v(" "),r("ul",[r("li",[e._v("训练过程")])]),e._v(" "),r("p",[e._v("在这个过程中，大模型通过调整其参数——包括权重和偏置——来捕捉这些复杂的模式和规律。训练完成后，这些参数就固定下来，代表了模型对数据的理解。")]),e._v(" "),r("ul",[r("li",[e._v("推理过程")])]),e._v(" "),r("p",[e._v("经过训练后，大模型不仅能记住训练数据中的具体内容，还能根据学到的模式推理出新的信息。例如，当我们给模型输入“"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E9%A9%AC%E9%A1%BF&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("马顿"),r("OutboundLink")],1),e._v("是人，人会死”时，模型可以根据学到的模式推理出“马顿会死”这样的结论。 但语聊中其实可以没有马顿这个人，你可以输入任何一个不存在，捏造的事物，让模型去推演这个模式，它能成功。")]),e._v(" "),r("p",[e._v("但模型的理解，和人类不一定一样，我们千万不要强行套用用人的模式，如果你把它带入太多人类思维，会失望。但如果你忽略掉模型学习到的特有能力，肯定会错过很多东西。这里有点玄学，没有理论，全都是"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%AE%9E%E8%B7%B5%E5%87%BA%E7%9C%9F%E7%9F%A5&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("实践出真知"),r("OutboundLink")],1),e._v("的结果。这个玄学的问题，即使是顶级的 AI 学者也是充满争议的，但不管怎么样，我们可以从"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%AE%9E%E7%94%A8%E4%B8%BB%E4%B9%89&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("实用主义"),r("OutboundLink")],1),e._v("出发解决工作的问题，包括我们后面讲的提示工程。")]),e._v(" "),r("h3",{attrs:{id:"问题讨论"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#问题讨论"}},[e._v("#")]),e._v(" 问题讨论")]),e._v(" "),r("p",[e._v("有奖问答: 模型真的是随机和概率的吗？（竟然没有人拿到 99 元的奖品《人工智能-现代方法》~~）")]),e._v(" "),r("ul",[r("li",[e._v("GPT采样的随机性")])]),e._v(" "),r("p",[e._v("推理输出的结果是一个数学矩阵，理论上输入数据相同，参数不变的情况下，应该永远一样。但最终数学矩阵要转换成现实世界的真实文本。这一过程涉及到一些随机采样方法，使得每次生成的文本可能有所不同。")]),e._v(" "),r("ul",[r("li",[e._v("贪心/波束算法")])]),e._v(" "),r("p",[e._v("首先，我们来讲讲"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("贪心算法"),r("OutboundLink")],1),e._v("。贪心算法会直接选择概率最高的词汇，这样可以保证每次输出都是一致的。然而，这种方法缺乏多样性，生成的文本可能显得单调。")]),e._v(" "),r("p",[e._v("gpt2 的出现，直接让更随机的采用方式，变成了 SOTA 业界最佳水平。后面随机采样成了主流，这一做法到底是不是"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("最佳实践"),r("OutboundLink")],1),e._v("，也是有争议的，有人认为波束算法的问题还是数据训练国产导致的。")]),e._v(" "),r("ul",[r("li",[e._v("温度缩放")])]),e._v(" "),r("p",[e._v("接下来是温度缩放。温度参数决定了生成文本的多样性。温度越低，logit向量会被压缩得越尖锐，高概率的词汇更容易被选择。0温度相当于退化成贪心算法。温度越高，生成的文本越随机，适合生成更加丰富多样的内容。")]),e._v(" "),r("ul",[r("li",[e._v("top-k 采样")])]),e._v(" "),r("p",[e._v("最后是 top-k 采样。这种方法限制了词汇表的选择范围，只从概率最高的k个词中进行选择。k值越大，选择空间越多，生成的文本也就越不确定。")]),e._v(" "),r("p",[e._v("在这个过程中，模型会根据输入生成一个输出"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%90%91%E9%87%8F%E7%9F%A9%E9%98%B5&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("向量矩阵"),r("OutboundLink")],1),e._v("。这个矩阵通过logit转换后，再映射到"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E8%AF%8D%E6%B1%87%E8%A1%A8&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("词汇表"),r("OutboundLink")],1),e._v("，生成可能的词汇。通过不同的采样策略，比如温度缩放和"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=top-k&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("top-k"),r("OutboundLink")],1),e._v("，我们可以得到不同的输出文本。")]),e._v(" "),r("p",[r("img",{attrs:{src:"https://pic4.zhimg.com/80/v2-3377040bb7cc1553ede19aa37c6e0817_720w.webp",alt:""}})]),e._v(" "),r("p",[e._v("简易版 gpt 推理")]),e._v(" "),r("p",[e._v("这里我想表达的是，训练后参数是固定的，在推理环节只是做"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("矩阵乘法"),r("OutboundLink")],1),e._v("，相同输入，的推理的中间数学输出是确定的， 不存在随机和概率问题。")]),e._v(" "),r("p",[e._v("我们看到的不确定性是来源于输出矩阵转现实世界文本的最后环节，刻意设计了一个概率选词算法，原因仅仅这样效果，文本更流畅。 模型的内在知识不存在不确定性。")]),e._v(" "),r("p",[e._v("之前的其他算法比如贪心算法，波束算法等效果不佳，openai 在 gpt2 中使用问题和 top-k 获得了更好效果。仅仅是目前的一个工程实践。")]),e._v(" "),r("h2",{attrs:{id:"提示工程-is-all-your-need"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#提示工程-is-all-your-need"}},[e._v("#")]),e._v(" 提示工程 is all your need")]),e._v(" "),r("p",[e._v("这个梗借用了 Attention Is All You Need。对于我们的大部分应用场景， 提示工程几乎是最重要的选择。")]),e._v(" "),r("p",[e._v("少量的业务知识，比如只是几百几千的内容，给到足够上下文是成本最低，性价比最高的方案。")]),e._v(" "),r("p",[e._v("微调通常只适合大量垂直且固有的业务知识。针对特定场景，在基础模型上微调，会让通用能力下降，这个是已知的问题。这种又叫遗忘性灾难，后面的数据会对前面学到的知识产生影响。除非你的数据足够大，足够优质，抵消对原始能力的损失。")]),e._v(" "),r("p",[e._v("微调且无法快速适应业务知识变化， 对于我们的大部分场景是"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E6%8A%95%E5%85%A5%E4%BA%A7%E5%87%BA%E6%AF%94&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("投入产出比"),r("OutboundLink")],1),e._v("非常低的事情。")]),e._v(" "),r("h3",{attrs:{id:"精心构造提示词"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#精心构造提示词"}},[e._v("#")]),e._v(" 精心构造提示词")]),e._v(" "),r("p",[e._v("首先，通过精心构造提示词，我们可以更好地还原被压缩的数据中的相关性。")]),e._v(" "),r("h3",{attrs:{id:"一次性输出的局限性"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#一次性输出的局限性"}},[e._v("#")]),e._v(" 一次性输出的局限性")]),e._v(" "),r("p",[e._v("其次，模型的一次性输出往往只提供有限的知识。")]),e._v(" "),r("p",[e._v("这里想表达的是，模型是认识这个世界的相关性，"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("提示工程"),r("OutboundLink")],1),e._v("就是去挖掘和利用这个数据相关性。")]),e._v(" "),r("p",[e._v("且模型训练的上下文有限，意味着学习到的相关性是有限的，通过工程化组合可以解决这个问题。")]),e._v(" "),r("p",[e._v("这个工作的价值并不会随着模型性能提高而没有用，三个诸葛亮还是比三个臭皮匠厉害。提示工程不会因为底层模型能力的提高，而失效，相反会让应用场景性能变得更好。")]),e._v(" "),r("h2",{attrs:{id:"一个分层"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#一个分层"}},[e._v("#")]),e._v(" 一个分层")]),e._v(" "),r("p",[e._v("就好像用 typescript 语言很容易学，你可以很快用它写一个 hello word。")]),e._v(" "),r("p",[e._v("但也用 typescript 语言搭建复杂的工程系统工程不简单。vscode 这个软件就是一个基于 typescript 构建的大型工程。")]),e._v(" "),r("p",[e._v("提示工程类似。自然语言提示词虽然很容易学，可以快速写一个翻译指令。 但搭建一个基于 LLM 的复杂的人机交互工程并不容易。")]),e._v(" "),r("h3",{attrs:{id:"l1提示工程-问答"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#l1提示工程-问答"}},[e._v("#")]),e._v(" L1提示工程-问答")]),e._v(" "),r("p",[e._v("一问一答，一个具体任务由一次模型回答解决。")]),e._v(" "),r("p",[e._v("我本来是想以『简单提示工程』作为副标题，但这些一问一答的提示工程『不能算简单』，而是最底层。关于提示工程技巧，我们不展开，大家从去年一年应该都接受过各种信息。我们要看这种提示工程的落地场景。")]),e._v(" "),r("p",[e._v("常见应用落地场景:")]),e._v(" "),r("p",[e._v("聊天，翻译，"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("代码生成"),r("OutboundLink")],1),e._v("，文字总结。")]),e._v(" "),r("p",[e._v("充当双语词典：“请充当帮助用户学习的词典，把用户给你的内容翻译成中文，给出发音、词语解释、双语示例和词源”。")]),e._v(" "),r("p",[e._v("目前调研的大部分英语学习软件都接入了"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=llm&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("llm"),r("OutboundLink")],1),e._v("，在 openai chatgpt3.5 刚刚出来的时候甚至出现过纯 llm 组成的英语学习软件，整个站点功能全部由大模型提示词组成。每个不同的模块就是一个独立的提示词，这种软件在 gpt 出现后大量涌现，极大的降低的英语软件的开发成本。")]),e._v(" "),r("p",[r("img",{attrs:{src:"https://pic2.zhimg.com/80/v2-567c2f1f1ae519912e47ff54411986a7_720w.webp",alt:""}})]),e._v(" "),r("h3",{attrs:{id:"l2-提示工程-组合"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#l2-提示工程-组合"}},[e._v("#")]),e._v(" L2 提示工程-组合")]),e._v(" "),r("p",[e._v("有时候任务很复杂，一个问题来回解决不了，或者解决的效果很差，怎么办。")]),e._v(" "),r("p",[e._v("想象我们解决一个问题，如果你直接灵感爆发，可以张口而来，比如：【这个需求还不容易，"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%B0%B1%E8%BF%99%E4%B9%88%E5%AE%9A%E4%BA%86&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("就这么定了"),r("OutboundLink")],1),e._v("】~。 结局很可能是导致大大的延期，并且和产品预期不符合。")]),e._v(" "),r("p",[e._v("可以更理性的思考方式： 这个需求需要哪些同学参与，涉及哪些工具，我应该选择和谁合作？ 这个思路对吗，然后我分别再按这些思路尝试。复杂的提示工程也是这样的。")]),e._v(" "),r("p",[e._v("我们可与把大模型假设成一个被关在房间里的人。假设每次这个人都会忘记之前说的话。 每次你只能给它一个问题，并且可以带上之前的历史内容。")]),e._v(" "),r("p",[e._v("就这样： 精心构造提示词、通过 N个问题，让模型模拟人类推理和行动的过程。")]),e._v(" "),r("p",[e._v("来让我们来看应该典型的向模型多次提问方式。")]),e._v(" "),r("p",[e._v("将设我们的任务是让大模型『帮忙生成"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%9B%9B%E5%A4%A7%E9%93%B6%E8%A1%8C&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("四大银行"),r("OutboundLink")],1),e._v("的最近 3 年的贷款利率，并生成图表』。我们可以下面一系列提示词组合。")]),e._v(" "),r("p",[e._v("这个场景我们假定每次都是独立的向模型提一个请求，同时假定有程序逻辑处理中介过程，但可以忽略细节。")]),e._v(" "),r("p",[e._v("2.1第一个提示词：")]),e._v(" "),r("blockquote",[r("p",[e._v("从现在开始你充当一个专家，你拥有调用不同工具的权限，每个工具都由它的作用，你必须模拟像人类一样思考。"),r("br"),e._v("\n下面是你拥有的工具列表："),r("br"),e._v(" "),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("搜索引擎"),r("OutboundLink")],1),e._v("：可以调用网络查询事实内容，参数是查询的“关键” 字。"),r("br"),e._v("\n图表生成：可以生成各种图统计图标，你需要把数据按 xx 格式给它。")]),e._v(" "),r("p",[e._v("每次收到用户的问题，你都需要先思考，再采取行动。以如下格式返回： 思考：我应该如何做这件事情呢，仔细思考它。"),r("br"),e._v("\n行动：我应该采取的方式是什么，可以调用其中的工具"),r("br"),e._v("\n观察：采取行动后观察到的结果是什么。"),r("br"),e._v("\n... ... 你应该不断循环以上步骤，直到你认为最终完成任务"),r("br"),e._v("\n最终答案：……")])]),e._v(" "),r("p",[e._v("【注：...... 有 5 次调用细节，字太多，中间已省略……】")]),e._v(" "),r("p",[e._v("可以发现我们 5 次独立的调用模型，最终完成了这个任务。这里的核心点是什么呢？ 我们运用提示词技巧，让模型模拟人一样的思考方式，给出行动轨迹，程序再识别行动轨迹，最终得到结果。这实际上就是所谓的 reAct 技术简化版介绍，R 代码推理，A代表路径，就是让模型自己推理，生成路径。实际的情况会更复杂，我们不展开讨论。")]),e._v(" "),r("p",[e._v("我们看到了提示工程不仅仅是简单的提问，还可以把多个问题有巧妙的组合起来，让模型解决问题。这是大模型能够影响显示世界，变成强大生产力的关键。人类现实世界有大量的工作、任务、决策经验都是由文本记在案，而模型正好通过训练学习到了这些文字续写的能力，我们反过来加以利用，就变成了经验复用。")]),e._v(" "),r("p",[r("img",{attrs:{src:"https://pica.zhimg.com/80/v2-4016d3f38809eb3feaf813f19db2a27a_720w.webp",alt:""}})]),e._v(" "),r("p",[e._v("L2 类应用的场景由很多，比如 RAG 信息检索、图表生成、PPT 生成、复杂任务等，任何尝试使用外部工具调用来增强 LLM 能力的都可以使用此类提示工程技术。")]),e._v(" "),r("h3",{attrs:{id:"l3提示工程-架构"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#l3提示工程-架构"}},[e._v("#")]),e._v(" L3提示工程-架构")]),e._v(" "),r("p",[r("img",{attrs:{src:"https://pic3.zhimg.com/80/v2-add03399004d1a956f4978069290322a_720w.webp",alt:""}})]),e._v(" "),r("p",[e._v("贴一个海外基于大模型做各种生态工具的图。")]),e._v(" "),r("p",[e._v("我们刚刚已经聊到 L2 级别的提示工程是可以通过多次提问，不断诱导模型生成解决问题的路径，对于一个问题的解决可能需要重复调用很多次才能完成。")]),e._v(" "),r("p",[e._v("试想下，如果一种更复杂的现实世界任务，比如请完成产品的这个需求，需求链接如下...要求完成编码，并通过测试，将代码 push 到仓库。")]),e._v(" "),r("p",[e._v("这样的一个任务对于人类而言都是有挑战的，它通过设计需求理解，git 仓库拉去，代码编写，"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E6%B5%8B%E8%AF%95%E7%94%A8%E4%BE%8B&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("测试用例"),r("OutboundLink")],1),e._v("编写，流水线部署，发布等研发流程。")]),e._v(" "),r("p",[e._v("如果我们继续尝试用上面的这种 reAct 方案，会发现要完成这项工作，可能准备几十个工具，并且把每个工具调用的推理、行动路径拼接到模型上下文，将会极大影响模型性能。")]),e._v(" "),r("p",[e._v("在 LLM 实践中，人们发现，即使是非常强的模型，让它一次专注理解并处理一个简单的任务，效果更佳。")]),e._v(" "),r("p",[e._v("为了解决这个问题，将一项复杂的工作，拆分成不同的模块，每个模块由一个独立的模型去解决，每个模型都拥有独立的上下文、并配置自己专属的工具。这些模型最终通过一个中介代理转发，这种模式就叫 Agent 。")]),e._v(" "),r("p",[e._v("Agent 可以代理更精细的模型分工，提示词可以对每个模块，甚至每次微小任务进行独立优化设计，reAct 只是被其使用的一种局部的技术而已。这样导致的问题则是调用会消耗更多 token，一项复杂的工作，可能会调用几十次，甚至上百次的 LLM 。")]),e._v(" "),r("p",[e._v("我为什么要把 Agent 作为提示工程标题一部呢？ 我要强调的是 虽然涉及 Agent 模型微调，以速度优化，但在不考虑响应速度的背景下，Agent 工程大部分工作将是围绕提示工程及其"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("组合优化"),r("OutboundLink")],1),e._v("进行。")]),e._v(" "),r("p",[e._v("我们今天不会介绍 Agent 细节，Agent 有各种各样的架构组合，业界和学术界的方案也层出不穷，大家可以基于兴趣自行探索。（也可与参考我的历史博文）")]),e._v(" "),r("p",[e._v("L3 提示工程——性能案例")]),e._v(" "),r("p",[e._v("SWE-bench 是一个"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("基准测试"),r("OutboundLink")],1),e._v("，用于评估从 GitHub 收集的真实软件问题上的"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("大型语言模型"),r("OutboundLink")],1),e._v("。词比基于纯 gpt4 高出好几倍。")]),e._v(" "),r("p",[r("img",{attrs:{src:"https://pica.zhimg.com/80/v2-25f805a04b23cb98d3071ae46c35bcde_720w.webp",alt:""}})]),e._v(" "),r("h2",{attrs:{id:"容易点"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#容易点"}},[e._v("#")]),e._v(" 容易点")]),e._v(" "),r("ul",[r("li",[r("p",[r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E5%BF%AB%E9%80%9F%E5%8E%9F%E5%9E%8B&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("快速原型"),r("OutboundLink")],1),e._v("：大模型再复杂的调用都是一条一条 LLM api 组成，快速原型不复杂。我们很多原型都能在基本甚至半天搞定。")])]),e._v(" "),r("li",[r("p",[e._v("有大量工具可以辅助： 从开发者视角，到小白视角，有大量工具可以辅助。")])])]),e._v(" "),r("p",[r("img",{attrs:{src:"https://pica.zhimg.com/80/v2-c58496eddf2deb624739af6b8b440e04_720w.webp",alt:""}})]),e._v(" "),r("p",[e._v("这是我之前一个调研，可参考历史博文")]),e._v(" "),r("p",[e._v("——除 langchain 外，任何复杂的 L3 级别的构建都不推荐使用这类工具，因为 L3 架构需要大量定制化调优，而这类工具通常是有自己的一套逻辑。")]),e._v(" "),r("h2",{attrs:{id:"挑战点"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#挑战点"}},[e._v("#")]),e._v(" 挑战点")]),e._v(" "),r("p",[e._v("● 提示词优化")]),e._v(" "),r("p",[e._v("提示词优化需要找到足够多的案例，才能反馈出你的效果 ，这里是需要花大量时间才能逐步提高。虽然有自动提示词这种工具，但想要达到不错的仍然是看人类专家。")]),e._v(" "),r("p",[e._v("● 组件交互")]),e._v(" "),r("p",[e._v("假设每个微小的提示词调整好了，组成成为一个更大的整体系统， 我们可能会效果不佳，因为每个局部的提示词是独立的。 必需考虑到全局的效果。这种全局的效果调整，非常消耗时间和 token 资源。")]),e._v(" "),r("p",[e._v("● 效果优化")]),e._v(" "),r("p",[e._v("可以用 20% 的时间快速构建 80% 的功能，但必须花剩下 80的时间来提高剩下 20% 的效果。这里其就是我们上面讲的点，在不考虑效果，确实可以用20% 的时间，搭建个 77，88，但最终是需要花 80%的时间，才能让系统正常上线的。 这里可以借鉴 linkedin 经验，他们花了1 个月快速出 demo ，但最终耗时 6 个月才大规模面向用户。")]),e._v(" "),r("p",[e._v("里面很多细节，比如为了让大模型更好理解接口，他们把全部的接口设计成给人类用的，和给大模型用的，后者面向大模型更友好，然后再这两者之间建立映射。")]),e._v(" "),r("p",[e._v("还有，比如我们的 json 解析通常会由于模型返回的格式不标准而异常，linkedin 编写一个非常巨大函数的把能够想得到的全部异常格式自动修复，做防御性解析。最终让解析报错率从5 % 降低到 0.005% ，极大的减少了重试的次数。")]),e._v(" "),r("p",[e._v("● 反馈评估")]),e._v(" "),r("p",[e._v("如果没有自动化工具评估，整个 LLM 工程系统很难得到最终反馈，我们目前大多靠人肉和用户反馈调研。")]),e._v(" "),r("p",[e._v("● 性能优化：")]),e._v(" "),r("p",[e._v("这里一个是底层的 LLM 响应优化，目前最主要的就是量化，缓存，gpu 加速，我们会设计比较少。")]),e._v(" "),r("p",[e._v("另一个应用层，可以通过流式、缓存、提示工程，及交互等提高体验。")]),e._v(" "),r("p",[e._v("举个流式例子：比如还有阿里钉钉为了可以让返回的 json 可以提前渲染前端 UI 组件，采用了流式提前解析， 在 json 没有完全返回的情况就进行自动补全， 渲染出 UI 轮廓和部分数据。")]),e._v(" "),r("p",[e._v("提示工程也和性能相关，因为不同的提示词，返回的 token 数量不一样，消耗的时间也会与差异。 这里 linkedin 有一个有趣的例子。他们尝试在一些场景深度使用 COT 上线后，由于用户量大，马上带来了机器负载问题。反正，调整后，负载发生了变化。")]),e._v(" "),r("p",[e._v("当然，这些场景主要是面向海量 C 端用户需要采取的措施，我们内部工具系统可以酌情考虑，根据用户反馈和实际需要来做。")]),e._v(" "),r("p",[e._v("对于我们而言，目前最大的困难点还是在前面 3 个点。也是最基础和不可忽略的提示工程调优。")]),e._v(" "),r("h2",{attrs:{id:"没有金科玉律"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#没有金科玉律"}},[e._v("#")]),e._v(" 没有金科玉律")]),e._v(" "),r("p",[e._v("在结尾，我们来再次分享一个个人的想法，大模型、特别是工程化和应用场景落地是当下非常新的东西。")]),e._v(" "),r("p",[e._v("我们没有太多过去的经验，业界的标准和方案都不断在更新迭代，以至于它没有像传统行业那样太多的最佳实践可以参考和指导，不存在 java 最佳实践、前端最佳实践，也没有经典的 24 种"),r("a",{attrs:{href:"https://zhida.zhihu.com/search?q=%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F&zhida_source=entity&is_preview=1",target:"_blank",rel:"noopener noreferrer"}},[e._v("设计模式"),r("OutboundLink")],1),e._v("。一切都是全新的。")]),e._v(" "),r("p",[e._v("这对我们而言既是挑战，也是机遇，我们并不需要被目前的这些已有技术优化、产品形态等思维约束，特别是提示工程，这里没有金科玉律。在将大模型技术和产品结合起来的路上，每个人都可以提出自己新的想法、去尝试新的方案，这能让自身和行业都得到繁荣和和发展。")]),e._v(" "),r("p",[e._v("注：狭义的提示工程指一对一提问优化； 广义上任何基于 LLM 提示工程构造的方案都属于这个范畴，上文 3 层架构是笔者构造的一个概念而已。")])])}),[],!1,null,null,null);r.default=v.exports}}]);